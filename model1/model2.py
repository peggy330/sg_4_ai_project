# -*- coding: utf-8 -*-
"""model2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kcyCOSPRXIhp4fX69sSI94-ZVyLhDX4p

#Model2
거시경제지표+뉴스감성분석 → 섹터 ETF 가격 예측

Step1. 거시경제지표
* 거시경제지표 데이터 수집 : 모델1과 동일
"""

pip install fredapi

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
from fredapi import Fred

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
from fredapi import Fred

# Define the list of tickers and their corresponding yfinance symbols or FRED series IDs
tickers = {
    "S&P 500": "^GSPC",
    "Dow Jones Industrial Average": "^DJI",
    "NASDAQ Composite Index": "^IXIC",
    "Russell 2000 Index": "^RUT",
    "MSCI World Index": "URTH",
    "FTSE 100": "^FTSE",
    "Nikkei 225": "^N225",
    "Hang Seng Index": "^HSI",
    "Shanghai Composite Index": "000001.SS",
    "Euro Stoxx 50": "^STOXX50E",
    "DAX Index": "^GDAXI",
    "CAC 40": "^FCHI",
    "S&P/ASX 200": "^AXJO",
    "BSE Sensex": "^BSESN",
    "Nifty 50": "^NSEI",
    "US 10Y Treasury Yield": "DGS10",
    "US 2Y Treasury Yield": "DGS2",
    "Federal Funds Rate": "DFF",
    "Dollar Index": "DTWEXBGS",
    "EUR/USD Exchange Rate": "DEXUSEU",
    "USD/JPY Exchange Rate": "DEXJPUS",
    "GBP/USD Exchange Rate": "DEXUSUK",
    "USD/CNY Exchange Rate": "DEXCHUS"
}

# Define the start and end dates
start_date = "2019-01-01"
end_date = "2023-12-31"

# Adjust the start date to ensure the interpolation window
start_date_adjusted = (datetime.strptime(start_date, '%Y-%m-%d') - timedelta(days=7)).strftime('%Y-%m-%d')

fred = Fred(api_key='ec0293b2fb9d336968dfd854c381389a')

# Function to get data from yfinance
def get_yfinance_data(ticker, start, end):
    data = yf.download(ticker, start=start, end=end)
    data = data[['Close']]
    return data

# Function to get data from FRED
def get_fred_data(ticker, start, end):
    data = fred.get_series(ticker, start, end)
    return pd.DataFrame(data, columns=[ticker])

# Initialize an empty dictionary to hold dataframes
data_frames = {}

# Fetch data for each ticker
for name, ticker in tickers.items():
    try:
        if ticker:
            if ticker.startswith('D') or ticker in ['USD3MTD156N', 'DFF', 'CPIAUCSL', 'PPIACO', 'UNRATE', 'A191RL1Q225SBEA']:
                data_frames[name] = get_fred_data(ticker, start_date, end_date)
            else:
                data_frames[name] = get_yfinance_data(ticker, start_date, end_date)
        else:
            print(f"No valid ticker for {name}")
    except Exception as e:
        print(f"Error fetching data for {name} ({ticker}): {e}")

# Function to calculate 7-day change, volatility, and store interpolated values
def calculate_change_and_volatility(df):
    results = []
    df_interpolated = df.interpolate(method='linear')

    for i in range(len(df_interpolated) - 7):
        window = df_interpolated.iloc[i:i+8].copy()

        start_value = window.iloc[0]
        end_value = window.iloc[-1]
        change = end_value - start_value
        volatility = window.max() - window.min()

        # Determine if the value was interpolated or original
        original_value = df.iloc[i].values[0]
        interpolated_value = df_interpolated.iloc[i].values[0]
        value = interpolated_value if pd.isnull(original_value) else original_value

        results.append({
            'date': df.index[i],
            'value': value,
            'seven_day_change': change.values[0],
            'seven_day_volatility': volatility.values[0]
        })

    return pd.DataFrame(results)

# Create a dictionary to hold the results
change_volatility_frames = {}

# Calculate change and volatility for each dataframe
for name, df in data_frames.items():
    print(f"Processing {name}")
    change_volatility_frames[name] = calculate_change_and_volatility(df)

# Example of accessing the data
for name, df in change_volatility_frames.items():
    print(f"{name} change and volatility data:")
    print(df.head())

# Save each dataframe to a separate CSV file (optional)
for name, df in change_volatility_frames.items():
    safe_name = name.replace(' ', '_').replace('&', 'and').replace('/', '_')
    try:
        df.to_csv(f"/content/drive/MyDrive/AI캡스톤/거시경제/{safe_name}_change_volatility.csv", index=False)
    except Exception as e:
        print(f"Error saving {name} data: {e}")

"""Step2. 섹터별 ETF 가격 데이터"""

import os

# ETF 목록
etf_list = [
    "XLK", "VGT", "XLF", "VFH", "XLV", "VHT", "XLE", "VDE",
    "XLI", "VIS", "XLY", "VCR", "XLP", "VDC", "XLC", "VOX",
    "XLU", "VPU", "XLRE", "VNQ", "BND", "AGG", "LQD", "HYG",
    "IEF", "GLD", "IAU", "SLV", "USO", "DBO", "VNQ", "REET",
    "IYR", "UUP", "FXE", "FXY", "FXB", "FXC"
]

# 데이터 저장 경로
save_path = "/content/drive/MyDrive/AI캡스톤/sector"

# 폴더가 존재하지 않으면 생성
if not os.path.exists(save_path):
    os.makedirs(save_path)

# 시작일과 종료일 설정
start_date = "2013-01-01"
end_date = "2023-12-31"

# 각 ETF의 데이터를 다운로드하고 저장
for etf in etf_list:
    data = yf.download(etf, start=start_date, end=end_date)
    df = data[['Close']].reset_index()
    df.columns = ['Date', 'Close']  # 칼럼 이름 재설정

    file_path = os.path.join(save_path, f"{etf}_price_data.csv")
    df.to_csv(file_path, index=False)
    print(f"{etf} 데이터 저장 완료: {file_path}")

import os
save_path = "/content/drive/MyDrive/AI캡스톤/sector"

# ETF 목록
etf_list = [
    "XLK", "VGT", "XLF", "VFH", "XLV", "VHT", "XLE", "VDE",
    "XLI", "VIS", "XLY", "VCR", "XLP", "VDC", "XLC", "VOX",
    "XLU", "VPU", "XLRE", "VNQ", "BND", "AGG", "LQD", "HYG",
    "IEF", "GLD", "IAU", "SLV", "USO", "DBO", "VNQ", "REET",
    "IYR", "UUP", "FXE", "FXY", "FXB", "FXC"
]

for etf in etf_list:
    file_path = os.path.join(save_path, f"{etf}_price_data.csv")
    df = pd.read_csv(file_path)

    # 'Date' 컬럼을 datetime 형식으로 변환
    df['Date'] = pd.to_datetime(df['Date'])

    # 7일 후의 날짜 계산
    df['Date_After_7_Days'] = df['Date'] + pd.Timedelta(days=7)

    # 7일 후의 가격을 가져오기 위해 merge 수행
    df_merged = pd.merge(df, df[['Date', 'Close']], left_on='Date_After_7_Days', right_on='Date', suffixes=('', '_After_7_Days'))

    # 7일 후 가격의 비율 계산
    df_merged['7_Day_Percent_Change'] = ((df_merged['Close_After_7_Days'] / df_merged['Close']) - 1) * 100

    # 필요한 컬럼만 유지
    df_final = df_merged[['Date', 'Close', '7_Day_Percent_Change']]

    # 다시 저장
    df_final.to_csv(file_path, index=False)
    print(f"{etf} 데이터 업데이트 완료: {file_path}")

"""Step3. 뉴스데이터 감성분석"""

!pip install selenium

"""섹터별 ETF 주요 뉴스 크롤링"""

etf_news_df1 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202301.csv', encoding='cp949')
etf_news_df2 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202302.csv', encoding='cp949')
etf_news_df3 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202303.csv', encoding='cp949')
etf_news_df4 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202304.csv', encoding='cp949')
etf_news_df5 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202305.csv', encoding='cp949')
etf_news_df6 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202306.csv', encoding='cp949')
etf_news_df7 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202307.csv', encoding='cp949')
etf_news_df8 = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_news/NASDAQ_RSS_IFO_202308.csv', encoding='cp949')

total_etf_news = pd.concat([etf_news_df1, etf_news_df2, etf_news_df3, etf_news_df4, etf_news_df5,
                           etf_news_df6, etf_news_df7, etf_news_df8])

!pip install KoreaNewsCrawler

#2019~2023년 뉴스데이터 크롤링

from korea_news_crawler.articlecrawler import ArticleCrawler

Crawler = ArticleCrawler()
Crawler.set_category('politics', 'economy', 'society', 'living_culture', 'world', 'IT_science', 'opinion')
Crawler.set_date_range("2019-01-01", "2023-12-31")
Crawler.start()

"""뉴스 감성분석"""

!pip install models

import pandas as pd
import numpy as np
import matplotlib as plt
import seaborn as sns
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import MinMaxScaler
from datetime import timedelta
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader, Dataset

# 감성분석에 필요한 모델 설치
!pip install -U textblob

# 감성분석 라이브러리 불러오기
import nltk
nltk.download('punkt')
from textblob import TextBlob
from nltk.sentiment.vader import SentimentIntensityAnalyzer

!pip install konlpy

import nltk
nltk.download('vader_lexicon')

news_sa_df = total_etf_news.drop(columns=['url_ifo'])

def detScore(num):
    if num is None:
        return None  # 또는 다른 기본값 또는 처리 로직을 사용할 수 있습니다.
    elif num < -0.5:
        return -2
    elif num >= -0.5 and num < 0:
        return -1
    elif num == 0.0:
        return 0
    elif num > 0 and num <= 0.5:
        return 1
    else:
        return 2

sia = SentimentIntensityAnalyzer()

total_etf_news['score'] = None
total_etf_news['predict'] = None

news_list = total_etf_news.loc[:, ['news_smy_ifo']] # 뉴스 헤드라인 조회
for index, row in news_list.iterrows():
    text = row['news_smy_ifo']
    score = sia.polarity_scores(text)['compound']

    # 'score' 컬럼에 계산된 점수 저장
    news_list.at[index, 'score'] = score

# 'predict' 컬럼 생성
def detScore(num):
    if num < -0.5:
        return -2
    elif num >= -0.5 and num < 0:
        return -1
    elif num == 0.0:
        return 0
    elif num > 0 and num <= 0.5:
        return 1
    else:
        return 2

news_list['predict'] = news_list['score'].apply(detScore)

# DataFrame 확인


news_list=news_list.drop(columns=['news_smy_ifo']) #중복생략

df_fin=pd.concat([total_etf_news, news_list], axis=1)

stock=pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/etf_type.csv',index_col=0)
df_fin['sector'] = ""#빈 컬럼 생성

for index, row in df_fin.iterrows():
    symbol = row['tck_iem_cd']
    matching_sector = stock[stock['Symbol'] == symbol]['IndustryCode'].values
    if len(matching_sector) > 0:
        df_fin.at[index, 'sector'] = matching_sector[0]

#t 일자의 특정 ETF의 뉴스 감성 분석 결과
total_etf_news['logit'] = np.log(total_etf_news)

news_sa_dataset = np.zeros(len(total_etf_news)*len(etf_list))
news_sa_dataset['date'] = [date * len(etf_list) for date in df{'date'}]
news_sa_dataset['logit_score']=0
news_sa_dataset['n_t'] = 0
news_sa_dataset['N'] = 0
news_sa_dataset['etf'] = etf_list * len(total_etf_news)

for index, row in total_etf_news.iterrows():
  condition = (news_sa_dataset['date']==row['date']) & (news_sa_dataset['etf']==row['sector'])
  news_sa_dataset[condition, 'logit_score'] += row['logit']
  news_sa_dataset[condition, 'n_t'] += 1
  news_sa_dataset[condition, 'N'] = news_sa_dataset[news_sa_dataset['etf']==row['sector'], 'n_t'].sum()

if news_sa_dataset['N'] != 0:
  news_sa_dataset['intensity'] = np.log(1 + news_sa_dataset['n_t']/news_sa_dataset['N'])
else :
  news_sa_dataset['intensity'] = 0

news_sa_dataset['fin_score'] = news_sa_dataset['logit_score'] * news_sa_dataset['intensity']

news_sa_dataset.to_csv('/content/drive/MyDrive/AI캡스톤/model2/news_sa.csv')

"""Step4. 섹터별 ETF 가격 예측 모델
* LSTM 모델
"""

""" LSTM stock price prediction: stacked LSTM """

# import libraries
import numpy as np
import pandas as pd
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# read the csv file
etf_price = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model1/merged_data.csv')
news_sa = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/news_sa.csv')
macro = pd.read_csv('/content/drive/MyDrive/AI캡스톤/model2/macro_econmic.csv')

stock_data = pd.concat([etf_price, news_sa,macro], axis=1 ) #통합 dataset
dates = pd.to_datetime(stock_data['Date'])
stock_data = stock_data.loc[stock_data['Date'] >= datetime(2023,1,1), :]

# normalize the dataset
scaler = StandardScaler()
scaler = scaler.fit(stock_data)
stock_data_scaled = scaler.transform(stock_data)

# split to train data and test data
n_train = int(0.9*stock_data_scaled.shape[0])
train_data_scaled = stock_data_scaled[0: n_train]
train_dates = dates[0: n_train]

test_data_scaled = stock_data_scaled[n_train:]
test_dates = dates[n_train:]
# print(test_dates.head(5))

# data reformatting for LSTM
pred_days = 1  # prediction period
seq_len = 7   # sequence length = past days for future prediction.
input_dim = 3  # input_dimension = ['Open', 'High', 'Low', 'Close', 'Volume']

trainX = []
trainY = []
testX = []
testY = []

for i in range(seq_len, n_train-pred_days +1):
    trainX.append(train_data_scaled[i - seq_len:i, 0:train_data_scaled.shape[1]])
    trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, 0])

for i in range(seq_len, len(test_data_scaled)-pred_days +1):
    testX.append(test_data_scaled[i - seq_len:i, 0:test_data_scaled.shape[1]])
    testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, 0])

trainX, trainY = np.array(trainX), np.array(trainY)
testX, testY = np.array(testX), np.array(testY)

# print(trainX.shape, trainY.shape)
# print(testX.shape, testY.shape)

# LSTM model
model_lstm = Sequential()
model_lstm.add(LSTM(64, input_shape=(trainX.shape[1], trainX.shape[2]), # (seq length, input dimension)
               return_sequences=True))
model_lstm.add(LSTM(32, return_sequences=False))
model_lstm.add(Dense(trainY.shape[1]))

model_lstm.summary()

# specify your learning rate
learning_rate = 0.01
# create an Adam optimizer with the specified learning rate
optimizer = Adam(learning_rate=learning_rate)
# compile your model using the custom optimizer
model_lstm.compile(optimizer=optimizer, loss='mse')

# Try to load weights
try:
    model_lstm.load_weights('./save_weights/lstm_weights.h5')
    print("Loaded model weights from disk")
except:
    print("No weights found, training model from scratch")
    # Fit the model
    history = model_lstm.fit(trainX, trainY, epochs=30, batch_size=32,
                    validation_split=0.1, verbose=1)
    # Save model weights after training
    model_lstm.save_weights('./save_weights/lstm_weights.h5')

    plt.plot(history.history['loss'], label='Training loss')
    plt.plot(history.history['val_loss'], label='Validation loss')
    plt.legend()
    plt.show()


# prediction
prediction = model_lstm.predict(testX)
print(prediction.shape, testY.shape)

# generate array filled with means for prediction
mean_values_pred = np.repeat(scaler.mean_[np.newaxis, :], prediction.shape[0], axis=0)

# substitute predictions into the first column
mean_values_pred[:, 0] = np.squeeze(prediction)

# inverse transform
y_pred = scaler.inverse_transform(mean_values_pred)[:,0]
print(y_pred.shape)

# generate array filled with means for testY
mean_values_testY = np.repeat(scaler.mean_[np.newaxis, :], testY.shape[0], axis=0)

# substitute testY into the first column
mean_values_testY[:, 0] = np.squeeze(testY)

# inverse transform
testY_original = scaler.inverse_transform(mean_values_testY)[:,0]
print(testY_original.shape)

# plotting
plt.figure(figsize=(14, 5))

# plot original 'Open' prices
plt.plot(dates, etf_price, color='green', label='Original Open Price')

# plot actual vs predicted
plt.plot(test_dates[seq_len:], testY_original, color='blue', label='Actual Open Price')
plt.plot(test_dates[seq_len:], y_pred, color='red', linestyle='--', label='Predicted Open Price')
plt.xlabel('Date')
plt.ylabel('Open Price')
plt.title('Original, Actual and Predicted Open Price')
plt.legend()
plt.show()

# Calculate the start and end indices for the zoomed plot
zoom_start = len(test_dates) - 50
zoom_end = len(test_dates)

# Create the zoomed plot
plt.figure(figsize=(14, 5))

# Adjust the start index for the testY_original and y_pred arrays
adjusted_start = zoom_start - seq_len

plt.plot(test_dates[zoom_start:zoom_end],
         testY_original[adjusted_start:zoom_end - zoom_start + adjusted_start],
         color='blue',
         label='Actual ETF Price')

plt.plot(test_dates[zoom_start:zoom_end],
         y_pred[adjusted_start:zoom_end - zoom_start + adjusted_start ],
         color='red',
         linestyle='--',
         label='Predicted ETF Price')

plt.xlabel('Date')
plt.ylabel('ETF Price')
plt.title('Zoomed In Actual vs Predicted ETF Price')
plt.legend()
plt.show()

"""* ResNet 모델"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, Add, Dense, GlobalAveragePooling1D
from tensorflow.keras.models import Model

# 데이터셋
def generate_data():
    X = pd.concat([news_sa,macro], axis=1 )
    y = etf_price
    return X, y

# ResNet 블록 생성
def resnet_block(x, filters, kernel_size=3):
    y = Conv1D(filters, kernel_size, padding='same')(x)
    y = BatchNormalization()(y)
    y = ReLU()(y)
    y = Conv1D(filters, kernel_size, padding='same')(y)
    y = BatchNormalization()(y)
    y = Add()([x, y])
    y = ReLU()(y)
    return y

# 입력 데이터
input_shape = (7, 1)  # 7일 동안의 데이터, 1개의 특성(예: 종가)
inputs = Input(shape=input_shape)

# ResNet 모델
x = Conv1D(64, 7, padding='same')(inputs)
x = BatchNormalization()(x)
x = ReLU()(x)
x = resnet_block(x, 64)
x = resnet_block(x, 64)
x = GlobalAveragePooling1D()(x)
outputs = Dense(1, activation='sigmoid')(x)

model_resnet = Model(inputs=inputs, outputs=outputs)

# 모델 컴파일
model_resnet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 데이터 생성 및 학습
X_train, y_train = generate_data()
model_resnet.fit(X_train, y_train, epochs=10, batch_size=32)

# 모델 요약 출력
model_resnet.summary()